#+begin_html
---
layout: post
title: Representación de números enteros
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
#+end_html

Hay quienes dicen (y suena verosimil) que el hecho de que usemos
cifras decimales para representar los números (es esa la forma en que
lo aprendemos en el colegio) está relacionado con que tengamos diez
dedos en las manos. Eso hace que resulte bastante fácil representar
los números del uno al diez solo usando los dedos. El problema se
presenta cuando el número que queremos representar es mayor
que 10. Como los números enteros son infinitos resulta muy poco
práctico dedicar una /cifra/ (es decir un símbolo gráfico) para cada
uno de ellos. Sólo nos acordaríamos algunos números, y para el resto
habría que ir a consultarlo, por ejemplo.

Por eso es que usamos más de una cifra para representar un
número. Así, para representar el 27 utilizamos los símbolos '2' y
'7'. La cifra de la izquierda es la "más significativa" y representa
en este caso /dos decenas/, la otra representa /siete unidades/. La
posición de la cifra indica de qué tipo de unidad se trata. Como las
cifras son decimales, en nuestro ejemplo se trata de una numeración
decimal. Entonces la i-ésima posición (comenzando de derecha a
izquierda, siendo 0 la primera posición) representa $10^i$ y la cifra
indica cuántas veces sumamos esa potencia. Más claramente:

\[
 n = \sum_{i=0}^k c_i b^i
\] 

donde $n$ es un número natural, $k$ es la cantidad de cifras (o de
/dígitos/) utilizadas para su representación, $c_i$ es la cifra que
está en la posición $i$ y $b$ es la "base" de la representación: 10
para decimal, 2 para binario, etc.

# Teniendo en cuenta esto, parece que tal vez hubiese sido más útil usar
# como base el número 6 en lugar del 10 como base de nuestro sistema de
# numeración, si la idea era representar los número con las manos. Con
# 6 se puede representar fácilmente hasta el número 35, si usamos cada
# mano como un dígito distinto (en lugar de usar ambas como uno
# decimal. Aunque pensándolo mejor podemos usar cada mano para
# representar del 0 al 9 independientemente si levantamos los dedos
# desde el pulgar hasta el meñique para representar del 0 al 5, y los
# bajamos también del pulgar hasta el anular -dejando levantados los que
# así lo estén- para representar del 6 al 9).

Parar representar cada una de las cifras, lo que necesitamos es que
haya una cantidad de símbolos suficiente. Pero obviamente para que sea
práctico, además su uso tiene que ser fácil distinguirlos (o dificil
confundirlos). Por ejemplo, hay relojes analógicos que representan los
segundos con una aguja que apunta a una circunferencia dividida en 60
partes para representar cada número entre 0 y 59, pero a veces no es
tan simple (sobre todo en aquellos donde el movimiento de esta aguja
es continuo) decidir qué segundo exacto está representando (al menos
de hacerlo en menos de un segundo) y parece mejor aproximar ese
número.


# todo: agregar nota bigit.
En las computadoras se usan digitos binarios. La palabra /bit/
proviene de la contracción de /binary digit/. Además, a diferencia de
la escritura a mano, los números en la computadora tienen una
precisión finita, es decir, tienen una cantidad fija de bits.  Como un
solo bit es poca información (sólo alcanza para representar dos
números), se utilizan "palabras" de varios bits contiguos. Así, por
ejemplo, si agrupamos 8 bits obtenemos un /byte/.

 En los [[https://software.intel.com/en-us/articles/intel-sdm][manuales de intel]], por ejemplo, se usan los nombres
siguientes:

|-------------------+----------|
| /byte/            | 8 bits   |
| /word/            | 16 bits  |
| /double word/     | 32 bits  |
| /quadword/        | 64 bits  |
| /double quadword/ | 128 bits |

Estas convenciones no son universalmente adoptadas. Por ejemplo, en el
[[https://www-cs-faculty.stanford.edu/~knuth/taocp.html][libro de Knuth]] se usan en cambio:

|-------------------+----------|
| /byte/            | 8 bits   |
| /wyde/            | 16 bits  |
| /tetrabyte/       | 32 bits  |
| /octabyte/        | 64 bits  |


Así como un bit puede representar 2 números, dado que agregando un bit
a cualquier tira de bits duplicamos la cantidad de números que podemos
representar, podemos concluir que en una tira de $n$ bits tenemos
$2^n$ números. 

Por ejemplo, en un byte hay 256. Naturalmente, representamos el 0 como
$0000\  0000_2$ (es decir, 8 ceros binarios, dejando un espacio para que
sea más fácil de leer). Si lo que queremos es representar números
enteros no negativos (o "sin signo" como se lo suele llamar) en un
byte, seguramente representaremos los números entre 0 y 255. Este
último de esta forma: $1111\  1111_2$. Para números más grandes
tendremos que usar tipos de datos también más grandes.


Para representar números enteros (es decir *con* signo, incluyendo
negativos), se utilizan cuatro distintas maneras.

*Signo + magnitud*

Una forma posible consiste en destinar un bit para representar el
signo del número, por ejemplo, el bit que se encuentra más a la
izquierda. En este caso, los bits $1111\  1111$ ya no serían el número
256 sino el -127, mientras que el -1 seía $1000 0001$. Por otra parte,
el 0 tiene dos representaciones distintas.


*Exceso a m*

Esta representación guarda a un número $n$ como $n + 2^{k-1}$, donde
$m = 2^{k-1}$ y $k$, de nuevo, es la cantidad de bits utilizados. Así,
si tenemos un byte, el número $-1$ será representado con 
$-1 + 2^7 = -1 + 128 = 127$.

*Complemento a uno*


